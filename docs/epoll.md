# Epoll 机制及其应用

## 内核态和用户态 —— 理解 I/O

要理解 I/O，首先要理解操作系统的权限管理。

- 内核态 (Kernel Space)：操作系统的核心，拥有最高权限，可以直接操作硬件（如网卡、硬盘、内存）。

- 用户态 (User Space)：程序（比如 webserver）运行的地方，权限受限，不能直接操作硬件。

当网卡收到网络数据时，数据首先进入**内核**管理的内存缓冲区。程序（用户态）想要读取这些数据，必须发起一个系统调用（如 `read`）。操作系统为了安全，不会让用户直接访问内核内存，而是将数据**从内核缓冲区复制到用户缓冲区**。这就是**数据拷贝**。

## 阻塞 I/O

假设服务器要处理客户端连接（Socket）。Socket 在操作系统眼中就是一个文件描述符（FD）。

- **场景：阻塞I/O**：（线程）去读取一个 Socket（`read(fd)`）。
	- **如果此时网卡还没收到数据**：操作系统会把线程挂起（让它睡觉，从 CPU 运行队列移除）。
	- **等待**：线程一直睡，直到数据来了。
	- **唤醒**：数据到了，操作系统把线程叫醒，拷贝数据，read 返回。


## 1. I/O 多路复用与 Epoll 的诞生背景

在构建高性能网络服务器时，最核心的挑战之一是如何在一个线程内高效地处理成百上千甚至上万个并发连接。传统的阻塞 I/O 模型在处理单个请求时会挂起线程，导致无法响应其他客户端；而多线程/多进程模型虽然解决了并发问题，但在海量连接下，上下文切换和内存开销会成为巨大的瓶颈。为了解决这个问题，操作系统引入了 I/O 多路复用技术（I/O Multiplexing），允许单个线程同时监控多个文件描述符（File Descriptors, FD）的状态。

早期的多路复用技术如 `select` 和 `poll` 虽然实现了这一目标，但它们存在显著的性能缺陷。它们在每次调用时都需要将整个 FD 集合从用户态拷贝到内核态，并且内核需要遍历整个集合来检查是否有事件发生，这导致其时间复杂度为 O(N)。随着连接数的增加，性能会呈线性下降。Linux 在 2.6 内核版本中引入了 `epoll`，它通过极其精妙的内核设计解决了这些问题，成为 Linux 平台下高性能网络编程的事实标准。

## 2. Epoll 的核心原理与内核机制

Epoll 的高效得益于其将“监听注册”与“事件等待”分离的设计理念。不同于 `select` 每次调用都要重复传递 FD 集合，`epoll` 在内核中维护了一个持久的数据结构（`eventpoll` 对象）。当我们调用 `epoll_create` 时，内核会创建一个 `eventpoll` 对象，该对象内部包含两个核心数据结构：一个是红黑树（Red-Black Tree），用于存储和索引所有被监控的 FD，保证了插入、删除和查找操作的 O(log N) 效率；另一个是双向链表（Ready List），仅用于存储当前已经“就绪”的事件。

Epoll 的真正的魔法在于其基于回调（Callback）的事件通知机制。当我们将一个 FD 通过 `epoll_ctl` 添加到红黑树中时，内核会为该 FD 注册一个回调函数。一旦网卡接收到数据并中断 CPU，中断处理程序在唤醒等待队列的同时，会触发这个回调函数，将该 FD 放入“就绪链表”中。因此，当用户程序调用 `epoll_wait` 时，内核不再需要遍历所有连接，而只需检查这个就绪链表是否为空。如果没有就绪事件，进程会挂起等待；一旦有节点被加入链表，`epoll_wait` 就会立即返回。这种机制使得 `epoll` 的性能只与“活跃连接数”相关，而不会受制于“总连接数”，在处理海量并发（C10K问题）时具有压倒性优势。

## 3. Epoll 在本项目中的架构设计与应用

在我们的 WebServer 项目中，`src/server/Server.cpp` 围绕 `epoll` 构建了一个经典的 Reactor 模式（事件驱动架构）。`Server` 类在初始化阶段通过 `_init_epoll` 函数调用 `epoll_create` 获取一个 epoll 实例句柄 `_epoll_fd`，并将服务器的监听 Socket（`_server_fd`）作为第一个监控对象加入 epoll 实例中，关注其 `EPOLLIN`（可读）事件。这标志着事件循环的起点。

项目的主循环位于 `Server::start()` 函数中，它不断调用 `epoll_wait` 阻塞等待事件发生。当 `epoll_wait` 返回时，我们会遍历返回的 `epoll_event` 数组。如果就绪的 FD 是 `_server_fd`，说明有新的客户端连接请求，此时服务器调用 `accept` 建立连接，并将新生成的客户端 FD 设置为非阻塞模式（Non-blocking），随后通过 `_add_to_epoll` 将其加入监控，初始关注 `EPOLLIN` 事件，准备接收客户端请求。

对于已建立的客户端连接，服务器根据事件类型进行状态流转。当 `EPOLLIN` 事件触发时，`_handle_client_data` 函数读取客户端发送的 HTTP 请求报文。值得注意的是，本项目采用了 epoll 的默认模式——水平触发（Level Triggered, LT）。这意味着只要接收缓冲区中还有数据未读完，epoll 就会不断通知我们。当请求读取完毕并生成响应后，服务器并不立即发送，而是通过 `_modify_epoll` 函数修改该 FD 的监听事件，将关注点从纯粹的读取（`EPOLLIN`）变为读取与写入（`EPOLLIN | EPOLLOUT`）。

当 socket 发送缓冲区可写时，`EPOLLOUT` 事件被触发，程序进入 `_handle_client_write` 函数发送 HTTP 响应数据。这种“读-处理-写”的分离设计保证了服务器不会因为向慢速客户端发送大数据而阻塞整个线程。一旦响应数据发送完毕，服务器会通过 `epoll_ctl` 的 `EPOLL_CTL_DEL` 操作将该 FD 从 epoll 监控中移除，并关闭连接，从而完成一次完整的 HTTP 请求处理周期。这种严谨的生命周期管理确保了系统资源能够被及时回收，避免了文件描述符泄漏的风险。


## Epoll 函数

- `int epoll_create(int size)` -> `epoll_fd`:
	- 在内核开辟一块内存区域，存 `eventpoll` 结构体。
	- 初始化epoll的红黑树(存监控的socket)和就绪链表(双向链表，存就绪的事件)
	- size 参数现在已经被内核忽略，大于0即可

- `int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)`
	- 将server_fd 加入 epoll 监控
	- 参数`op`： EPOLL_CTL_ADD (添加), EPOLL_CTL_MOD (修改), EPOLL_CTL_DEL (删除)
	- 参数 `event`: 一个结构体，包含关心的事件（EPOLLIN | EPOLLOUT）和 用户数据（通常存 fd）
	- 内核动作： ADD：在红黑树插入节点，并注册回调函数 ep_poll_callback。MOD：修改红黑树节点中的关注事件（如从“只读”改为“读写”）。

- `int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout)`
	- 内核动作： 检查 eventpoll 对象的就绪链表。如果链表为空：挂起当前线程（阻塞），直到超时或有事件触发回调。如果链表不空：将链表中的事件数据拷贝到用户态的 events 数组中。
	- 返回 就绪事件的数量 n。用户只需要遍历 events[0] 到 events[n-1]。